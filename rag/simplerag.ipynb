{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.', metadata={'source': 'speech.txt'})]\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "print(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\n",
      "\n",
      "Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\n",
      "\n",
      "…\n",
      "\n",
      "It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\n",
      "\n",
      "We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\n",
      "\n",
      "It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\n",
      "\n",
      "To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.\n"
     ]
    }
   ],
   "source": [
    "print(text_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://decodeai.in/posts/Linear-Regression/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"\\nLinear Regression\\nLinear Functions\\nLinear are equations with x and y values raised to the power of 1 (usually).\\nThey can be written in Standard Form:\\nAx + By = C\\nOr Slope-Intercept Form:\\ny = mx + b\\nNon-Linear Function\\nNon-Linear Equations have x and y values raised to the power of 2 or more.\\nThe functions form wavy lines of some kind but still pass the vertical line test.\\nOur goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis.\\n\\nFigure 1 : Model Representation\\n\\n\\\\[h_{\\\\theta }(X) = y\\\\]\\n\\n\\nLinear regression with one variable - Univariate linear regression.\\n\\n\\nCost Function\\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from X's and the actual output y's.\\n\\n\\\\[J\\\\left(\\\\theta_0, \\\\theta_1\\\\right)=\\\\frac{1}{2 m} \\\\sum_{i=1}^m\\\\left(\\\\hat{y}_i-y_i\\\\right)^2=\\\\frac{1}{2 m} \\\\sum_{i=1}^m\\\\left(h_\\\\theta\\\\left(x_i\\\\right)-y_i\\\\right)^2\\\\]\\n\\nTo break it apart, it is $\\\\frac{1}{2}$\\u200b $\\\\bar{x}$ where $\\\\bar{x}$ is the mean of the squares of $h_\\\\theta\\\\left(x_i\\\\right)-y_i$ , or the difference between the predicted value and the actual value.\\nThis function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $\\\\frac{1}{2}$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\\\frac{1}{2}$ term. The following image summarizes what the cost function does:\\n\\nFigure 2 : Cost Function Representation\\nCost Function - Intuition I\\nIf we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_\\\\theta\\\\left(x\\\\right)$ which passes through these scattered data points.\\n\\nOur objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J\\\\left(\\\\theta_0, \\\\theta_1\\\\right)$ will be 0. The following example shows the ideal situation where we have a cost function of 0.\\n\\n\\nFigure 3 : Cost Function Intuition I\\nWhen $\\\\theta_1\\u200b=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $\\\\theta_1$\\u200b=0.5, we see the vertical distance from our fit to the data points increase.\\n\\nFigure 4 : Cost Function Intuition I\\nThis increases our cost function to 0.58. Plotting several other points yields to the following graph:\\n\\nFigure 5 : Cost Function Intuition I\\nCost Function - Intuition II\\n\\nA contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line.\\n\\nAn example of such a graph is the one to the right below.\\n\\nFigure 6 : Cost Function Intuition II\\nTaking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J\\\\left(\\\\theta_0, \\\\theta_1\\\\right)$\\n and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $\\\\theta_0$ = 800 and $\\\\theta_1$ = -0.15. Taking another h(x) and plotting its contour plot, one gets the following graphs:\\n\\nFigure 7 : Cost Function Intuition II\\nWhen $\\\\theta_0$ = 360 and $\\\\theta_1$ = 0, the value of $J\\\\left(\\\\theta_0, \\\\theta_1\\\\right)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.\\n\\nFigure 8 : Cost Function Intuition II\\nThe graph above minimizes the cost function as much as possible and consequently, the result of $\\\\theta_1$ and $\\\\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’.\\n\\nGradient Descent\\n\\nWe have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in.\\n\\n\\n\\\\[h_{\\\\theta }(X) = \\\\theta_0 + \\\\theta_1X\\\\]\\n\\nImagine that we graph our hypothesis function based on its fields $\\\\theta_0$ and $\\\\theta_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.\\nWe put $\\\\theta_0$ on the x axis and $\\\\theta_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\\n\\nFigure 9 : Gradient Descent\\nWe will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum.  The red arrows show the minimum points in the graph.\\nThe way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.\\n\\nFor example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J\\\\left(\\\\theta_0, \\\\theta_1\\\\right)$. Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.\\n\\nThe gradient descent algorithm is:\\n\\n\\\\[\\\\theta_j := \\\\theta_j - \\\\alpha \\\\frac{\\\\partial}{\\\\partial \\\\theta_j} J(\\\\theta_0, \\\\theta_1)\\\\]\\n\\nThe derivatives of Gradient Descent Algorithm\\n\\n\\n\\n\\n\\n\\nReferences:\\n\\nSlope of a line\\nSlope from an Equation\\nHow to Find Slope from Two Points\\nGraphing Lines from Slope and y-intercept (y = mx + b)\\nMatching Graph to Equations (Simplifying Math)\\nDerivatives Made Easy! Power Rule\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://decodeai.in/posts/Linear-Regression/'})]\n"
     ]
    }
   ],
   "source": [
    "print(text_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression\n",
      "Linear Functions\n",
      "Linear are equations with x and y values raised to the power of 1 (usually).\n",
      "They can be written in Standard Form:\n",
      "Ax + By = C\n",
      "Or Slope-Intercept Form:\n",
      "y = mx + b\n",
      "Non-Linear Function\n",
      "Non-Linear Equations have x and y values raised to the power of 2 or more.\n",
      "The functions form wavy lines of some kind but still pass the vertical line test.\n",
      "Our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis.\n",
      "\n",
      "Figure 1 : Model Representation\n",
      "\n",
      "\\[h_{\\theta }(X) = y\\]\n",
      "\n",
      "\n",
      "Linear regression with one variable - Univariate linear regression.\n",
      "\n",
      "\n",
      "Cost Function\n",
      "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from X's and the actual output y's.\n",
      "\n",
      "\\[J\\left(\\theta_0, \\theta_1\\right)=\\frac{1}{2 m} \\sum_{i=1}^m\\left(\\hat{y}_i-y_i\\right)^2=\\frac{1}{2 m} \\sum_{i=1}^m\\left(h_\\theta\\left(x_i\\right)-y_i\\right)^2\\]\n",
      "\n",
      "To break it apart, it is $\\frac{1}{2}$​ $\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta\\left(x_i\\right)-y_i$ , or the difference between the predicted value and the actual value.\n",
      "This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $\\frac{1}{2}$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term. The following image summarizes what the cost function does:\n",
      "\n",
      "Figure 2 : Cost Function Representation\n",
      "Cost Function - Intuition I\n",
      "If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_\\theta\\left(x\\right)$ which passes through these scattered data points.\n",
      "\n",
      "Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J\\left(\\theta_0, \\theta_1\\right)$ will be 0. The following example shows the ideal situation where we have a cost function of 0.\n",
      "\n",
      "\n",
      "Figure 3 : Cost Function Intuition I\n",
      "When $\\theta_1​=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $\\theta_1$​=0.5, we see the vertical distance from our fit to the data points increase.\n",
      "\n",
      "Figure 4 : Cost Function Intuition I\n",
      "This increases our cost function to 0.58. Plotting several other points yields to the following graph:\n",
      "\n",
      "Figure 5 : Cost Function Intuition I\n",
      "Cost Function - Intuition II\n",
      "\n",
      "A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line.\n",
      "\n",
      "An example of such a graph is the one to the right below.\n",
      "\n",
      "Figure 6 : Cost Function Intuition II\n",
      "Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J\\left(\\theta_0, \\theta_1\\right)$\n",
      " and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $\\theta_0$ = 800 and $\\theta_1$ = -0.15. Taking another h(x) and plotting its contour plot, one gets the following graphs:\n",
      "\n",
      "Figure 7 : Cost Function Intuition II\n",
      "When $\\theta_0$ = 360 and $\\theta_1$ = 0, the value of $J\\left(\\theta_0, \\theta_1\\right)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.\n",
      "\n",
      "Figure 8 : Cost Function Intuition II\n",
      "The graph above minimizes the cost function as much as possible and consequently, the result of $\\theta_1$ and $\\theta_0$ tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’.\n",
      "\n",
      "Gradient Descent\n",
      "\n",
      "We have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in.\n",
      "\n",
      "\n",
      "\\[h_{\\theta }(X) = \\theta_0 + \\theta_1X\\]\n",
      "\n",
      "Imagine that we graph our hypothesis function based on its fields $\\theta_0$ and $\\theta_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.\n",
      "We put $\\theta_0$ on the x axis and $\\theta_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n",
      "\n",
      "Figure 9 : Gradient Descent\n",
      "We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum.  The red arrows show the minimum points in the graph.\n",
      "The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter α, which is called the learning rate.\n",
      "\n",
      "For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter α. A smaller α would result in a smaller step and a larger α results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J\\left(\\theta_0, \\theta_1\\right)$. Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.\n",
      "\n",
      "The gradient descent algorithm is:\n",
      "\n",
      "\\[\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\\]\n",
      "\n",
      "The derivatives of Gradient Descent Algorithm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References:\n",
      "\n",
      "Slope of a line\n",
      "Slope from an Equation\n",
      "How to Find Slope from Two Points\n",
      "Graphing Lines from Slope and y-intercept (y = mx + b)\n",
      "Matching Graph to Equations (Simplifying Math)\n",
      "Derivatives Made Easy! Power Rule\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('ManoharPalanisamy_Resume.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Manohar Palanisamy\\n♂phone+91-9819876078 /envel⌢pemanoharpalanisamy@icloud.com /userwww.decodeai.in /linkedinmanoharpalanisamy\\nWORK EXPERIENCES\\nSenior Associate - Machine learning Dec 2018 – Dec 2023\\nSevasys Technologies Bangalore\\n•Contributed to POC projects in Generative AI, implementing end-to-end solutions with OpenAI, Langchain,\\nGemini AI and Open source models like llama3.\\n•Collaborated cross-functionally to brainstorm, prototype, and iterate ML solutions, gaining practical experience in\\nreal-world applications.\\n•Integrated Gen AI models into existing systems, optimizing performance and scalability for production deployment.\\n•Contributed to team efforts in research and development, staying abreast of the latest advancements in\\ngenerative AI technologies.\\nExecutive Software Developer Jan 2017 – Dec 2018\\nQuantumLink Communications Pvt Ltd Mumbai\\n•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking\\nsystem to monitor salesperson activities.\\n•Responsible for handling feature enhancement and bugfixing related to our products which also included understanding\\nthe requirements and preparing design documents and Impact Analysis.\\n•Worked on an email system with full knowledge of network and email protocols, ensuring timely delivery and quality, all\\nin line with the client’s quality standards.\\nTECHNICAL SKILLS\\nGen AI Frameworks: OpenAI, Langchain Ecosystem, HuggingFace, Google Generative AI,\\nArtificial Neural Networks: Perceptron, MLP, CNN, RNN, LSTM, GRU, BRNN, Autoencoders, GAN, Residual Net,\\nTransformers, ChatGPT, Llama3, Gemini Pro, Gemini Pro Vision.\\nML Model And Algorithms: Linear Regression, Logistic Regression, SVM, Gradient Descent, SGD, minibatch GD,\\nAdam etc.\\nCoding Skills: Scripting and web development including Python (NumPy, scikit-learn) , Streamlit, Java, FastAPI, HTTP\\nServer, Mail protocols and DNS server\\nDeep Learning Frameworks: TensorFlow, Keras, PyTorch\\nNatural Language Processing: NLTK (Natural Language Toolkit), spaCy\\nModel Evaluation: Cross-validation, ROC curve analysis, Confusion matrix, Grid search, Hyperparameter tuning\\nDeployment: Docker, AWS ECR, Kubernetes, Huggingface Spaces, Vercel, CloudflareCDN\\nCloud Platforms: AWS SageMaker, BedRock, AppRunner, EC2 and S3\\nSoftware Development: Agile methodology, Continuous Integration/Continuous Deployment (CI/CD), GitHub Actions\\nDatabase Management: MySQL, MongoDB, Chroma DB, FAISS, Pinecone Vector Database\\nVersion Control: Git\\nCERTIFICATIONS\\nGenerative AI with Large Language Models [MAVEJZ9P9AV5] Sep 2023 – Apr 2024\\nDeepLearning.AI And Amazon Web Services Coursera.org\\nDeep Learning Specialization [EKJCGU5G2NXF] July 2023 – Feb 2024\\nDeepLearning.AI Coursera.org\\nMachine Learning Specialization [2CE3DQW8Z2NG] Jun 2023 – Feb 2024\\nStanford University And DeepLearning.AI Coursera.org\\nApplied AI with DeepLearning [DRENBUK43NW8] Dec 2021 – Jan 2022\\nIBM Coursera.org', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 0}), Document(page_content='PROJECTS\\nGemini Pro: Text-to-SQL Query Generation |Python, Gemini Pro, SQLite, Streamlit, Huggingface Spaces Live\\n•Developed an end-to-end project utilizing the Google Generative AI - Gemini Pro Model.\\n•Created a user-friendly interface using Streamlit for text-to-SQL query generation.\\n•Designed and implemented functionality to extract information from SQLite databases based on user input prompts.\\n•Utilized Huggingface Space deployment for seamless integration of AI models, making it accessible via the web at\\nhttps://huggingface.co/spaces/ManoharPalanisamy/TextToSQLGenerativeAI\\nChatGPT MCQ Generator |OpenAI API, chatgpt-3.5-turbo, Langchain, Streamlit UI, AWS EC2, Huggingface Spaces\\n•Created a modular MCQ Generator using the OpenAI API, chatgpt-3.5-turbo model, Sequential Langchain, and\\nStreamlit UI.\\n•Utilized advanced natural language processing capabilities to get user inputs and subject-specific data to generate\\nmultiple-choice questions (MCQs).\\n•Presented MCQs in a user-friendly table format via Streamlit UI and deployed the project on AWS EC2 for accessibility\\nand scalability.\\nGemini Pro Vision: The Future of Invoice Data Extraction |Python, Gemini Pro Vision, Huggingface Spaces Live\\n•Developed a cutting-edge solution to automate invoice processing using Google Generative AI\\nGemini Pro Vision Model.\\n•Leveraged image-to-text conversion and advanced NLP techniques to extract key information from invoices with\\nprecision and accuracy. I have Used the Streamlit interface for designing the application.\\n•Integrated Huggingface Spaces for deployment and management, making it accessible via the web at\\nhttps://huggingface.co/spaces/ManoharPalanisamy/InvoiceExtractor\\nImage Classifier |Python, FastAI , Render Live\\n•Developed an advanced image classifier using the ResNet50 neural network architecture from the Fastai deep learning\\nlibrary.\\n•Trained the model to classify images of Marvel Heroes into 12 distinct classes.\\n•Leveraged the powerful capabilities of Fastai for efficient training and model evaluation.\\n•Hosted the final trained model on Render, making it accessible via the web at https://project.decodeai.in\\nPERSONAL BLOG\\nwww.decodeai.in |JAM Stack, Jekyll, Cloudflare, Vercel, ImageKit.io, Umami Analytics, Giscus.app, Crisp Live\\n•Regularly publish articles on machine learning, deep learning, and other technology-related topics.\\n•Provide valuable resources and insights to fellow tech enthusiasts and learners.\\nPUBLICATION\\nFake Account Eliminator March 2015\\nNational Conference on Engineering Applications for Developing Smart Cities - NCEADS\\n•This software is designed to find and remove fake accounts on platforms like Facebook, Gmail, and Twitter in India.\\n•It uses the UIDAI (Unique Identification Authority of India) as a key part of its operation.\\n•India has many fake users on social media and email, so this tool helps to identify and eliminate them. It enforces a rule\\nwhere each person can only have one account, reducing problems for real users and the public.\\nEDUCATION\\nBachelor of Engineering in Computer Science May 2016\\nDhirajlal Gandhi College of Technology, Anna University Salem, Tamilnadu\\nLast update: May 1, 2024', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 1})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manohar Palanisamy\n",
      "♂phone+91-9819876078 /envel⌢pemanoharpalanisamy@icloud.com /userwww.decodeai.in /linkedinmanoharpalanisamy\n",
      "WORK EXPERIENCES\n",
      "Senior Associate - Machine learning Dec 2018 – Dec 2023\n",
      "Sevasys Technologies Bangalore\n",
      "•Contributed to POC projects in Generative AI, implementing end-to-end solutions with OpenAI, Langchain,\n",
      "Gemini AI and Open source models like llama3.\n",
      "•Collaborated cross-functionally to brainstorm, prototype, and iterate ML solutions, gaining practical experience in\n",
      "real-world applications.\n",
      "•Integrated Gen AI models into existing systems, optimizing performance and scalability for production deployment.\n",
      "•Contributed to team efforts in research and development, staying abreast of the latest advancements in\n",
      "generative AI technologies.\n",
      "Executive Software Developer Jan 2017 – Dec 2018\n",
      "QuantumLink Communications Pvt Ltd Mumbai\n",
      "•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking\n",
      "system to monitor salesperson activities.\n",
      "•Responsible for handling feature enhancement and bugfixing related to our products which also included understanding\n",
      "the requirements and preparing design documents and Impact Analysis.\n",
      "•Worked on an email system with full knowledge of network and email protocols, ensuring timely delivery and quality, all\n",
      "in line with the client’s quality standards.\n",
      "TECHNICAL SKILLS\n",
      "Gen AI Frameworks: OpenAI, Langchain Ecosystem, HuggingFace, Google Generative AI,\n",
      "Artificial Neural Networks: Perceptron, MLP, CNN, RNN, LSTM, GRU, BRNN, Autoencoders, GAN, Residual Net,\n",
      "Transformers, ChatGPT, Llama3, Gemini Pro, Gemini Pro Vision.\n",
      "ML Model And Algorithms: Linear Regression, Logistic Regression, SVM, Gradient Descent, SGD, minibatch GD,\n",
      "Adam etc.\n",
      "Coding Skills: Scripting and web development including Python (NumPy, scikit-learn) , Streamlit, Java, FastAPI, HTTP\n",
      "Server, Mail protocols and DNS server\n",
      "Deep Learning Frameworks: TensorFlow, Keras, PyTorch\n",
      "Natural Language Processing: NLTK (Natural Language Toolkit), spaCy\n",
      "Model Evaluation: Cross-validation, ROC curve analysis, Confusion matrix, Grid search, Hyperparameter tuning\n",
      "Deployment: Docker, AWS ECR, Kubernetes, Huggingface Spaces, Vercel, CloudflareCDN\n",
      "Cloud Platforms: AWS SageMaker, BedRock, AppRunner, EC2 and S3\n",
      "Software Development: Agile methodology, Continuous Integration/Continuous Deployment (CI/CD), GitHub Actions\n",
      "Database Management: MySQL, MongoDB, Chroma DB, FAISS, Pinecone Vector Database\n",
      "Version Control: Git\n",
      "CERTIFICATIONS\n",
      "Generative AI with Large Language Models [MAVEJZ9P9AV5] Sep 2023 – Apr 2024\n",
      "DeepLearning.AI And Amazon Web Services Coursera.org\n",
      "Deep Learning Specialization [EKJCGU5G2NXF] July 2023 – Feb 2024\n",
      "DeepLearning.AI Coursera.org\n",
      "Machine Learning Specialization [2CE3DQW8Z2NG] Jun 2023 – Feb 2024\n",
      "Stanford University And DeepLearning.AI Coursera.org\n",
      "Applied AI with DeepLearning [DRENBUK43NW8] Dec 2021 – Jan 2022\n",
      "IBM Coursera.org\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Manohar Palanisamy\\n♂phone+91-9819876078 /envel⌢pemanoharpalanisamy@icloud.com /userwww.decodeai.in /linkedinmanoharpalanisamy\\nWORK EXPERIENCES\\nSenior Associate - Machine learning Dec 2018 – Dec 2023\\nSevasys Technologies Bangalore\\n•Contributed to POC projects in Generative AI, implementing end-to-end solutions with OpenAI, Langchain,\\nGemini AI and Open source models like llama3.\\n•Collaborated cross-functionally to brainstorm, prototype, and iterate ML solutions, gaining practical experience in\\nreal-world applications.\\n•Integrated Gen AI models into existing systems, optimizing performance and scalability for production deployment.\\n•Contributed to team efforts in research and development, staying abreast of the latest advancements in\\ngenerative AI technologies.\\nExecutive Software Developer Jan 2017 – Dec 2018\\nQuantumLink Communications Pvt Ltd Mumbai\\n•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 0}), Document(page_content='QuantumLink Communications Pvt Ltd Mumbai\\n•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking\\nsystem to monitor salesperson activities.\\n•Responsible for handling feature enhancement and bugfixing related to our products which also included understanding\\nthe requirements and preparing design documents and Impact Analysis.\\n•Worked on an email system with full knowledge of network and email protocols, ensuring timely delivery and quality, all\\nin line with the client’s quality standards.\\nTECHNICAL SKILLS\\nGen AI Frameworks: OpenAI, Langchain Ecosystem, HuggingFace, Google Generative AI,\\nArtificial Neural Networks: Perceptron, MLP, CNN, RNN, LSTM, GRU, BRNN, Autoencoders, GAN, Residual Net,\\nTransformers, ChatGPT, Llama3, Gemini Pro, Gemini Pro Vision.\\nML Model And Algorithms: Linear Regression, Logistic Regression, SVM, Gradient Descent, SGD, minibatch GD,\\nAdam etc.', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 0}), Document(page_content='Transformers, ChatGPT, Llama3, Gemini Pro, Gemini Pro Vision.\\nML Model And Algorithms: Linear Regression, Logistic Regression, SVM, Gradient Descent, SGD, minibatch GD,\\nAdam etc.\\nCoding Skills: Scripting and web development including Python (NumPy, scikit-learn) , Streamlit, Java, FastAPI, HTTP\\nServer, Mail protocols and DNS server\\nDeep Learning Frameworks: TensorFlow, Keras, PyTorch\\nNatural Language Processing: NLTK (Natural Language Toolkit), spaCy\\nModel Evaluation: Cross-validation, ROC curve analysis, Confusion matrix, Grid search, Hyperparameter tuning\\nDeployment: Docker, AWS ECR, Kubernetes, Huggingface Spaces, Vercel, CloudflareCDN\\nCloud Platforms: AWS SageMaker, BedRock, AppRunner, EC2 and S3\\nSoftware Development: Agile methodology, Continuous Integration/Continuous Deployment (CI/CD), GitHub Actions\\nDatabase Management: MySQL, MongoDB, Chroma DB, FAISS, Pinecone Vector Database\\nVersion Control: Git\\nCERTIFICATIONS', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 0}), Document(page_content='Database Management: MySQL, MongoDB, Chroma DB, FAISS, Pinecone Vector Database\\nVersion Control: Git\\nCERTIFICATIONS\\nGenerative AI with Large Language Models [MAVEJZ9P9AV5] Sep 2023 – Apr 2024\\nDeepLearning.AI And Amazon Web Services Coursera.org\\nDeep Learning Specialization [EKJCGU5G2NXF] July 2023 – Feb 2024\\nDeepLearning.AI Coursera.org\\nMachine Learning Specialization [2CE3DQW8Z2NG] Jun 2023 – Feb 2024\\nStanford University And DeepLearning.AI Coursera.org\\nApplied AI with DeepLearning [DRENBUK43NW8] Dec 2021 – Jan 2022\\nIBM Coursera.org', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 0}), Document(page_content='PROJECTS\\nGemini Pro: Text-to-SQL Query Generation |Python, Gemini Pro, SQLite, Streamlit, Huggingface Spaces Live\\n•Developed an end-to-end project utilizing the Google Generative AI - Gemini Pro Model.\\n•Created a user-friendly interface using Streamlit for text-to-SQL query generation.\\n•Designed and implemented functionality to extract information from SQLite databases based on user input prompts.\\n•Utilized Huggingface Space deployment for seamless integration of AI models, making it accessible via the web at\\nhttps://huggingface.co/spaces/ManoharPalanisamy/TextToSQLGenerativeAI\\nChatGPT MCQ Generator |OpenAI API, chatgpt-3.5-turbo, Langchain, Streamlit UI, AWS EC2, Huggingface Spaces\\n•Created a modular MCQ Generator using the OpenAI API, chatgpt-3.5-turbo model, Sequential Langchain, and\\nStreamlit UI.\\n•Utilized advanced natural language processing capabilities to get user inputs and subject-specific data to generate\\nmultiple-choice questions (MCQs).', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 1}), Document(page_content='Streamlit UI.\\n•Utilized advanced natural language processing capabilities to get user inputs and subject-specific data to generate\\nmultiple-choice questions (MCQs).\\n•Presented MCQs in a user-friendly table format via Streamlit UI and deployed the project on AWS EC2 for accessibility\\nand scalability.\\nGemini Pro Vision: The Future of Invoice Data Extraction |Python, Gemini Pro Vision, Huggingface Spaces Live\\n•Developed a cutting-edge solution to automate invoice processing using Google Generative AI\\nGemini Pro Vision Model.\\n•Leveraged image-to-text conversion and advanced NLP techniques to extract key information from invoices with\\nprecision and accuracy. I have Used the Streamlit interface for designing the application.\\n•Integrated Huggingface Spaces for deployment and management, making it accessible via the web at\\nhttps://huggingface.co/spaces/ManoharPalanisamy/InvoiceExtractor\\nImage Classifier |Python, FastAI , Render Live', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 1}), Document(page_content='https://huggingface.co/spaces/ManoharPalanisamy/InvoiceExtractor\\nImage Classifier |Python, FastAI , Render Live\\n•Developed an advanced image classifier using the ResNet50 neural network architecture from the Fastai deep learning\\nlibrary.\\n•Trained the model to classify images of Marvel Heroes into 12 distinct classes.\\n•Leveraged the powerful capabilities of Fastai for efficient training and model evaluation.\\n•Hosted the final trained model on Render, making it accessible via the web at https://project.decodeai.in\\nPERSONAL BLOG\\nwww.decodeai.in |JAM Stack, Jekyll, Cloudflare, Vercel, ImageKit.io, Umami Analytics, Giscus.app, Crisp Live\\n•Regularly publish articles on machine learning, deep learning, and other technology-related topics.\\n•Provide valuable resources and insights to fellow tech enthusiasts and learners.\\nPUBLICATION\\nFake Account Eliminator March 2015\\nNational Conference on Engineering Applications for Developing Smart Cities - NCEADS', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 1}), Document(page_content='PUBLICATION\\nFake Account Eliminator March 2015\\nNational Conference on Engineering Applications for Developing Smart Cities - NCEADS\\n•This software is designed to find and remove fake accounts on platforms like Facebook, Gmail, and Twitter in India.\\n•It uses the UIDAI (Unique Identification Authority of India) as a key part of its operation.\\n•India has many fake users on social media and email, so this tool helps to identify and eliminate them. It enforces a rule\\nwhere each person can only have one account, reducing problems for real users and the public.\\nEDUCATION\\nBachelor of Engineering in Computer Science May 2016\\nDhirajlal Gandhi College of Technology, Anna University Salem, Tamilnadu\\nLast update: May 1, 2024', metadata={'source': 'ManoharPalanisamy_Resume.pdf', 'page': 1})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manohar Palanisamy\n",
      "♂phone+91-9819876078 /envel⌢pemanoharpalanisamy@icloud.com /userwww.decodeai.in /linkedinmanoharpalanisamy\n",
      "WORK EXPERIENCES\n",
      "Senior Associate - Machine learning Dec 2018 – Dec 2023\n",
      "Sevasys Technologies Bangalore\n",
      "•Contributed to POC projects in Generative AI, implementing end-to-end solutions with OpenAI, Langchain,\n",
      "Gemini AI and Open source models like llama3.\n",
      "•Collaborated cross-functionally to brainstorm, prototype, and iterate ML solutions, gaining practical experience in\n",
      "real-world applications.\n",
      "•Integrated Gen AI models into existing systems, optimizing performance and scalability for production deployment.\n",
      "•Contributed to team efforts in research and development, staying abreast of the latest advancements in\n",
      "generative AI technologies.\n",
      "Executive Software Developer Jan 2017 – Dec 2018\n",
      "QuantumLink Communications Pvt Ltd Mumbai\n",
      "•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents[:],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manohar Palanisamy\n",
      "♂phone+91-9819876078 /envel⌢pemanoharpalanisamy@icloud.com /userwww.decodeai.in /linkedinmanoharpalanisamy\n",
      "WORK EXPERIENCES\n",
      "Senior Associate - Machine learning Dec 2018 – Dec 2023\n",
      "Sevasys Technologies Bangalore\n",
      "•Contributed to POC projects in Generative AI, implementing end-to-end solutions with OpenAI, Langchain,\n",
      "Gemini AI and Open source models like llama3.\n",
      "•Collaborated cross-functionally to brainstorm, prototype, and iterate ML solutions, gaining practical experience in\n",
      "real-world applications.\n",
      "•Integrated Gen AI models into existing systems, optimizing performance and scalability for production deployment.\n",
      "•Contributed to team efforts in research and development, staying abreast of the latest advancements in\n",
      "generative AI technologies.\n",
      "Executive Software Developer Jan 2017 – Dec 2018\n",
      "QuantumLink Communications Pvt Ltd Mumbai\n",
      "•Developed a research-based web application called FieldSense. This project involved creating a Salesforce tracking\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the work experience of manohar\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "faiss_db = FAISS.from_documents(documents[:], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Management: MySQL, MongoDB, Chroma DB, FAISS, Pinecone Vector Database\n",
      "Version Control: Git\n",
      "CERTIFICATIONS\n",
      "Generative AI with Large Language Models [MAVEJZ9P9AV5] Sep 2023 – Apr 2024\n",
      "DeepLearning.AI And Amazon Web Services Coursera.org\n",
      "Deep Learning Specialization [EKJCGU5G2NXF] July 2023 – Feb 2024\n",
      "DeepLearning.AI Coursera.org\n",
      "Machine Learning Specialization [2CE3DQW8Z2NG] Jun 2023 – Feb 2024\n",
      "Stanford University And DeepLearning.AI Coursera.org\n",
      "Applied AI with DeepLearning [DRENBUK43NW8] Dec 2021 – Jan 2022\n",
      "IBM Coursera.org\n"
     ]
    }
   ],
   "source": [
    "query = \"what are certifications all done\"\n",
    "retireved_results=faiss_db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
